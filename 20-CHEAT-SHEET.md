# 20 - QUICK REFERENCE & DECISION GUIDE

## ğŸš€ Purpose
Fast lookup for common decisions, patterns, and quick references. Not a command cookbook - a decision support tool.

---

## ğŸ¯ Common Scenarios & Decisions

### **Scenario 1: "Which Metric Type Should I Use?"**

```
Decision Tree:

What are you measuring?
â”‚
â”œâ”€ Current state (can go up/down)
â”‚  Example: active_users, queue_size, temperature
â”‚  â†’ Use: GAUGE
â”‚    statsd.gauge('metric', value)
â”‚
â”œâ”€ Counting events
â”‚  Example: requests, orders, errors
â”‚  â”‚
â”‚  â”œâ”€ Need rate (per second)?
â”‚  â”‚  â†’ Use: COUNT + .as_rate() in query
â”‚  â”‚    statsd.increment('requests')
â”‚  â”‚    Query: sum:requests{*}.as_rate()
â”‚  â”‚
â”‚  â””â”€ Just total count?
â”‚     â†’ Use: COUNT
â”‚       statsd.increment('orders')
â”‚       Query: sum:orders{*}.as_count()
â”‚
â”œâ”€ Duration or size (need percentiles)
â”‚  Example: response_time, file_size
â”‚  â”‚
â”‚  â”œâ”€ Low volume (< 1K/sec)?
â”‚  â”‚  â†’ Use: DISTRIBUTION
â”‚  â”‚    statsd.distribution('response_time', ms)
â”‚  â”‚    Why: Full percentile flexibility
â”‚  â”‚
â”‚  â””â”€ High volume (> 1K/sec)?
â”‚     â†’ Use: HISTOGRAM
â”‚       statsd.histogram('response_time', ms)
â”‚       Why: More cost-effective
â”‚
â””â”€ Unique counts
   Example: unique_visitors, distinct_errors
   â†’ Use: SET (rare, usually use gauge with count distinct)
     statsd.set('visitors', user_id)
```

---

### **Scenario 2: "How to Tag This Metric?"**

```
Decision Framework:

Q1: How many unique values?
    < 100 â†’ OK to tag âœ…
    100-1000 â†’ Caution âš ï¸
    > 1000 â†’ DON'T TAG âŒ (use logs instead)
    
Q2: Can I group/bucket?
    user_id (million values) â†’ user_tier (3 values) âœ…
    specific_error_message â†’ error_type âœ…
    timestamp â†’ hour_of_day âœ…
    
Q3: Will I query by this?
    YES â†’ Tag it âœ…
    NO â†’ Don't add (reduce cardinality) âœ…
    
Q4: For debugging or analytics?
    Debugging â†’ Logs (high cardinality OK)
    Analytics â†’ Tags (low cardinality required)

Examples:

âœ… GOOD Tags:
  env:production              (3 values: dev, staging, prod)
  service:payment-api         (20 services in system)
  status:success              (2-3 values)
  user_tier:premium           (3 tiers)
  endpoint:/api/users         (~100 endpoints)
  region:us-east-1            (5 regions)

âŒ BAD Tags (High Cardinality):
  user_id:12345               (millions)
  request_id:abc123           (unique each request)
  timestamp:1234567890        (always changing)
  email:user@example.com      (millions)
  specific_error_msg          (thousands of variations)
```

---

### **Scenario 3: "Dashboard Widget Selection"**

```
What do you want to show?

â”Œâ”€ Single current value â†’ Query Value
â”‚  Examples: Current CPU: 67%, Active users: 850
â”‚
â”œâ”€ Trend over time â†’ Timeseries
â”‚  Examples: Request rate, Memory usage, Response time
â”‚
â”œâ”€ Ranking (top/bottom) â†’ Top List
â”‚  Examples: Top 10 hosts by CPU, Slowest 5 endpoints
â”‚
â”œâ”€ Distribution â†’ Distribution Widget
â”‚  Examples: Latency percentiles (p50/p95/p99)
â”‚
â”œâ”€ Pattern across dimensions â†’ Heatmap
â”‚  Examples: Errors by service Ã— time, Latency by region
â”‚
â”œâ”€ Live events â†’ Log Stream
â”‚  Examples: Real-time errors, Deployment logs
â”‚
â”œâ”€ Infrastructure overview â†’ Host Map
â”‚  Examples: CPU across all hosts (color-coded)
â”‚
â”œâ”€ Service dependencies â†’ Service Map
â”‚  Examples: Microservices topology (auto-generated from APM)
â”‚
â””â”€ Multiple metrics comparison â†’ Table
   Examples: Host comparison (CPU, RAM, Disk), Service SLOs
```

---

### **Scenario 4: "Query Aggregation Choice"**

```
Choose aggregation based on use case:

avg:
  âœ… Most common, general purpose
  âœ… CPU usage, memory, response time
  âœ… Smooths out spikes
  Example: avg:system.cpu.user{*}

sum:
  âœ… Counts, requests, bytes
  âœ… Total across all hosts/services
  Example: sum:http.requests{*}

max:
  âœ… Peak values, worst case
  âœ… Detect spikes
  Example: max:system.cpu.user{*} by {host}
         â†’ Find which host peaked

min:
  âœ… Lowest value, best case
  âœ… Available resources
  Example: min:system.disk.free{*}
         â†’ Least free disk space

pXX (percentiles):
  âœ… Latency, performance metrics
  âœ… Better than average for user experience
  Example: p95:trace.http.request.duration{*}
         â†’ 95% of requests faster than this
```

---

### **Scenario 5: "How Much to Sample?"**

```
Decision Matrix:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Type       â”‚ Volume   â”‚ Sample%  â”‚ Reason      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Error traces    â”‚ Any      â”‚ 100%     â”‚ Must catch  â”‚
â”‚ Slow traces     â”‚ Any      â”‚ 100%     â”‚ Debug perf  â”‚
â”‚ Normal traces   â”‚ Low      â”‚ 100%     â”‚ Affordable  â”‚
â”‚                 â”‚ Medium   â”‚ 10-20%   â”‚ Balance     â”‚
â”‚                 â”‚ High     â”‚ 1-5%     â”‚ Cost        â”‚
â”‚ Info logs       â”‚ Any      â”‚ 10%      â”‚ Not criticalâ”‚
â”‚ Debug logs      â”‚ Any      â”‚ 0%       â”‚ Never prod  â”‚
â”‚ Security logs   â”‚ Any      â”‚ 100%     â”‚ Compliance  â”‚
â”‚ Business logs   â”‚ Any      â”‚ 100%     â”‚ Analytics   â”‚
â”‚ Health checks   â”‚ High     â”‚ 0%       â”‚ Noise       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Implementation:

APM Sampling (in datadog.yaml):
apm_config:
  analyzed_spans:
    # Critical: always
    payment-service|*: 1.0
    auth-service|*: 1.0
    
    # Errors: always
    *|error: 1.0
    
    # Normal: sample based on volume
    web-api|*: 0.1        # 10% high traffic
    mobile-api|*: 0.2     # 20% medium traffic
    admin-api|*: 1.0      # 100% low traffic
    
    # Health: never
    *|health: 0.0

Log Sampling (in pipeline):
- Index 100%: status:(error OR critical)
- Sample 10%: status:info
- Exclude: status:debug, @endpoint:/health
```

---

### **Scenario 6: "Cost Optimization - Where to Start?"**

```
Priority Order (Impact vs Effort):

1. QUICK WINS (Do First):
   
   âœ… Exclude health check logs
      Impact: 20-40% log cost reduction
      Effort: 5 minutes
      How: Exclusion filter: @endpoint:/health
   
   âœ… Sample info logs to 10%
      Impact: 30-50% log cost reduction
      Effort: 10 minutes
      How: Sampling rule in log pipeline
   
   âœ… Remove high-cardinality tags
      Impact: Can save 50-90% metric cost
      Effort: 30 minutes (identify + fix)
      Example: user_id tag â†’ 1M metrics â†’ $50K/month
               Remove tag â†’ 100 metrics â†’ $500/month

2. MEDIUM EFFORT (Do Second):
   
   âœ… APM sampling configuration
      Impact: 30-50% APM cost
      Effort: 1 hour
      How: Configure analyzed_spans
   
   âœ… Reduce log retention
      Impact: 10-30% log cost
      Effort: 30 minutes
      How: 7 days vs 15 days
   
   âœ… Disable unused integrations
      Impact: 5-15% overall
      Effort: 1 hour (audit + disable)

3. ONGOING OPTIMIZATION:
   
   âœ… Regular usage dashboard review
      Frequency: Weekly
      Look for: Unexpected spikes, new high-cost services
   
   âœ… Tag cardinality monitoring
      Frequency: Monthly
      Alert on: Tags with > 1000 unique values
   
   âœ… Cost attribution by team
      Purpose: Accountability, chargeback

Cost Dashboard Query Examples:
- Metric cardinality: Check Metrics Summary page
- Log volume by service: sum:datadog.estimated_usage.logs.ingested_bytes{*} by {service}
- APM spans by service: sum:datadog.estimated_usage.apm.ingested_spans{*} by {service}
```

---

### **Scenario 7: "Agent Deployment Model Choice"**

```
Platform-Based Decision:

VMs/Bare Metal:
  â†’ Agent per host (standard)
  Install: Package manager (apt, yum)
  Config: /etc/datadog-agent/datadog.yaml

Docker (Single Host):
  â†’ Datadog Agent container
  Install: docker run datadog/agent
  Config: Environment variables
  Volumes: /var/run/docker.sock, /proc/, /sys/fs/cgroup/

Kubernetes:
  â†’ Cluster Agent + Node Agents (DaemonSet)
  Install: Helm chart (recommended)
  Config: values.yaml
  Why: Cluster-level metrics, efficient API usage

AWS Lambda:
  â†’ Serverless monitoring (no agent)
  Install: Lambda extension or Forwarder
  Why: Ephemeral, can't run persistent agent

Legacy/Mainframe:
  â†’ Agentless (API polling) or Proxy
  Why: Cannot modify legacy systems

Decision Summary:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Platform         â”‚ Model            â”‚ Method      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linux/Windows VM â”‚ Agent per host   â”‚ Package mgr â”‚
â”‚ Docker           â”‚ Agent container  â”‚ Docker run  â”‚
â”‚ Kubernetes       â”‚ Cluster + Daemon â”‚ Helm chart  â”‚
â”‚ Lambda           â”‚ Extension        â”‚ Layer       â”‚
â”‚ Legacy           â”‚ Agentless        â”‚ API poll    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Scenario 8: "Monitor Type Selection"**

```
What to alert on?

Metric threshold breach:
  â†’ Metric Monitor
  Example: CPU > 80%
  Query: avg:system.cpu.user{*} > 80

Log pattern:
  â†’ Log Monitor
  Example: More than 10 errors in 5 minutes
  Query: status:error service:payment-api
  Threshold: > 10 in 5 min

APM performance:
  â†’ APM Monitor
  Example: p95 latency > 500ms
  Query: p95:trace.http.request.duration{service:api} > 500

Multiple conditions (AND/OR):
  â†’ Composite Monitor
  Example: High CPU AND High Memory
  Formula: a && b

Change detection:
  â†’ Change Monitor
  Example: Traffic increased 50% vs 1 hour ago
  Query: avg:http.requests{*} increases > 50%

Unusual pattern:
  â†’ Anomaly Monitor
  Example: CPU behaving abnormally
  Algorithm: ML-based anomaly detection

Predict future issue:
  â†’ Forecast Monitor
  Example: Disk will be full in 1 week
  Query: forecast(avg:disk.used{*}, 'linear', 1w) >= 90

Decision Tree:
â”Œâ”€ Simple threshold â†’ Metric Monitor
â”œâ”€ Log pattern â†’ Log Monitor
â”œâ”€ Need AND/OR logic â†’ Composite Monitor
â”œâ”€ Detect change â†’ Change Monitor
â”œâ”€ Unusual behavior â†’ Anomaly Monitor
â””â”€ Predict future â†’ Forecast Monitor
```

---

## ğŸ” Query Quick Reference

### **Query Syntax Cheatsheet**

```bash
Format: <agg>:<metric>{<filters>} [by {<tags>}] [.function()]

# Basic
avg:system.cpu.user{*}
sum:http.requests{env:production}
max:database.connections{*} by {host}

# Filters
{env:production}                    # Single tag
{env:production,service:api}        # AND
{env:production OR env:staging}     # OR
{service:web-*}                     # Wildcard
{env:production,-service:test}      # NOT

# Functions
.as_rate()          # Count to rate (per second)
.as_count()         # Rate to count
.rollup(avg, 60)    # Average in 60s windows
timeshift(..., 86400)  # Compare to 1 day ago (86400s)
anomalies(..., 'basic', 2)  # Anomaly detection
forecast(..., 'linear', 1w)  # Forecast 1 week ahead
ewma_5(...)         # Exponential smoothing

# Arithmetic
(a - b) / a * 100   # Calculate percentage
a + b               # Sum metrics
a / b               # Divide metrics

# Examples
Error rate %:
(sum:http.errors{*} / sum:http.requests{*}) * 100

Free memory %:
(avg:system.mem.total{*} - avg:system.mem.used{*}) / avg:system.mem.total{*} * 100

Compare to yesterday:
avg:system.cpu.user{*}
timeshift(avg:system.cpu.user{*}, 86400)
```

---

### **Log Search Quick Reference**

```bash
# Basic search
service:web-api
status:error
env:production

# Combine (implicit AND)
service:web-api status:error

# Boolean operators
service:web-api AND status:error
status:(error OR warning)
service:web-api -status:info  # NOT

# Wildcards
service:web-*
message:*timeout*

# Numeric ranges
@http.response_time:[100 TO 500]
@http.response_time:>1000
@user.age:<18

# Facets
@user.id:12345
@http.status_code:500
-@error.type:*  # Doesn't have error.type

# Time
@timestamp:[now-1h TO now]

# Common patterns
# Find errors for specific user:
service:payment-api status:error @user.id:12345

# Find slow requests:
service:api @duration:>1000

# Find failed payments:
service:payment status:error @transaction.type:payment
```

---

## ğŸ·ï¸ Tagging Best Practices

### **Unified Service Tagging (Required)**

```yaml
# Always include (standard across all services):
env: production
service: payment-api
version: v1.2.3

Why:
- env: Filter by environment
- service: Group by service
- version: Correlate with deployments

How to set:
# Agent config
tags:
  - env:production
  - service:payment-api
  - version:v1.2.3

# Environment variables
DD_ENV=production
DD_SERVICE=payment-api
DD_VERSION=v1.2.3

# APM automatic
DD_SERVICE=payment-api DD_ENV=production DD_VERSION=v1.2.3 ddtrace-run python app.py
```

---

### **Additional Recommended Tags**

```yaml
Infrastructure:
  datacenter: us-east-1
  availability_zone: us-east-1a
  instance_type: t3.large
  host: web-server-01

Team/Org:
  team: backend
  squad: payments
  cost_center: engineering

Business:
  criticality: high
  customer_tier: premium

Banking-Specific:
  channel: internet-banking
  transaction_type: payment
  compliance_scope: pci
  business_unit: retail
```

---

## ğŸš¨ Troubleshooting Checklist

### **Agent Not Sending Data**

```bash
Step 1: Verify agent status
  sudo datadog-agent status
  â†’ Look for "Forwarder" section, should say "Running"

Step 2: Check connectivity
  curl -v https://api.datadoghq.com
  â†’ Should return 200 OK
  â†’ If fails: Firewall/proxy issue

Step 3: Verify API key
  sudo datadog-agent config | grep api_key
  â†’ Should match key in Datadog UI (Organization Settings â†’ API Keys)

Step 4: Check site setting
  sudo datadog-agent config | grep site
  â†’ US1: datadoghq.com
  â†’ EU1: datadoghq.eu
  â†’ Must match your login URL

Step 5: Check logs for errors
  sudo tail -f /var/log/datadog/agent.log
  â†’ Look for ERROR or WARN lines

Step 6: Run diagnostics
  sudo datadog-agent diagnose
  â†’ Auto-checks common issues

Step 7: Send flare (support bundle)
  sudo datadog-agent flare
  â†’ Sends diagnostics to Datadog support
```

---

### **High Metric Cardinality**

```bash
Problem: Metric count exploding, high bill

Identify:
  1. Go to Metrics â†’ Summary
  2. Sort by "# of Tags"
  3. Look for metrics with > 1000 timeseries

Root Cause:
  High-cardinality tag (user_id, request_id, timestamp)

Fix:
  Option 1: Remove high-cardinality tag
    âŒ tags: [user_id:12345]
    âœ… tags: [user_tier:premium]
  
  Option 2: Use logs instead
    Metrics: Aggregate data (user_tier, status)
    Logs: Specific data (user_id, request_id)
  
  Option 3: Bucket values
    âŒ response_time:245ms (unique each time)
    âœ… response_time_bucket:200-300ms (few buckets)

Prevention:
  - Review tag cardinality monthly
  - Alert on metrics with > 1000 timeseries
  - Code review: Check tags before deploying
```

---

### **Missing Logs from Containers**

```yaml
Checklist:

â–¡ Agent logs enabled?
  DD_LOGS_ENABLED=true

â–¡ Container log collection enabled?
  DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL=true

â–¡ Docker socket mounted?
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock:ro

â–¡ Correct permissions?
  Agent container user must be in docker group

â–¡ Container using stdout/stderr?
  â†’ Agent only collects stdout/stderr, not file logs

â–¡ Check agent status:
  docker exec dd-agent agent status
  â†’ Look for "Logs Agent" section

â–¡ Label filtering?
  Check if container excluded by label filters
  
â–¡ Log volume limits?
  Check if hit rate limits (10MB/s per container)
```

---

## ğŸ’° Cost Quick Reference

### **Pricing Summary (As of 2026)**

```
Infrastructure Monitoring:
  $15 per host per month

APM:
  $31 per host per month
  + $1.70 per million spans indexed

Log Management:
  $0.10 per GB ingested
  $1.27 per million log events indexed

Custom Metrics:
  $0.05 per metric per month

RUM:
  $1.50 per thousand sessions

Synthetic Monitoring:
  $5 per 10,000 API test runs

Network Performance Monitoring:
  $5 per host per month
```

---

### **Cost Calculation Examples**

```
Example 1: Small Startup (10 hosts)
â”œâ”€ Infrastructure: 10 Ã— $15 = $150
â”œâ”€ APM: 10 Ã— $31 = $310
â”œâ”€ Logs: 100GB Ã— $0.10 = $10
â””â”€ Total: ~$470/month

Example 2: Medium Company (100 hosts)
â”œâ”€ Infrastructure: 100 Ã— $15 = $1,500
â”œâ”€ APM: 100 Ã— $31 = $3,100
â”œâ”€ Logs: 1TB Ã— $0.10 = $100
â”œâ”€ Custom Metrics: 500 Ã— $0.05 = $25
â””â”€ Total: ~$4,725/month

Example 3: Banking (200 hosts, heavy monitoring)
â”œâ”€ Infrastructure: 200 Ã— $15 = $3,000
â”œâ”€ APM: 200 Ã— $31 = $6,200
â”œâ”€ Logs: 5TB Ã— $0.10 = $500
â”œâ”€ Log indexing: 100M events Ã— $1.27/M = $127
â”œâ”€ Custom Metrics: 2000 Ã— $0.05 = $100
â”œâ”€ RUM: 500K sessions Ã— $1.50/1K = $750
â”œâ”€ Synthetics: 100K tests Ã— $5/10K = $50
â””â”€ Total: ~$10,727/month
```

---

## ğŸ”— Essential Links

### **Datadog Sites**
```
US1 (default): https://app.datadoghq.com
US3:           https://us3.datadoghq.com
US5:           https://us5.datadoghq.com
EU1:           https://app.datadoghq.eu
```

### **Documentation**
```
Main Docs:     https://docs.datadoghq.com/
API Docs:      https://docs.datadoghq.com/api/
Learning:      https://learn.datadoghq.com/
Status:        https://status.datadoghq.com/
Agent GitHub:  https://github.com/DataDog/datadog-agent
```

---

## ğŸ“ Quick Decision Tables

### **When to Use Datadog vs Alternatives**

```
Choose Datadog if:
âœ… Want quick setup (< 1 week to production)
âœ… Small/medium team (< 20 DevOps engineers)
âœ… All-in-one solution needed
âœ… Have budget ($5K-$50K+/month)
âœ… Need 24/7 support
âœ… Cloud-native architecture

Choose Self-Hosted (Prometheus + Grafana + ELK) if:
âœ… Large DevOps team (10+ engineers)
âœ… Data must stay on-premise (strict sovereignty)
âœ… Very tight budget
âœ… Want full control & customization
âœ… Have time to maintain (2-3 FTE ongoing)

Choose New Relic if:
âœ… Simpler pricing preferred (user-based)
âœ… Strong mobile APM need
âœ… Less infrastructure monitoring needed
```

---

## ğŸ“Š Banking-Specific Quick Reference

### **Compliance Checklist**

```yaml
Data Sovereignty:
  â–¡ Use EU site for GDPR (app.datadoghq.eu)
  â–¡ Use US site for US data (app.datadoghq.com)
  â–¡ Configure data scrubbing (mask PII)
  â–¡ Document data flows

Security:
  â–¡ API keys in secrets manager (not git)
  â–¡ RBAC configured (least privilege)
  â–¡ SSO enabled (SAML)
  â–¡ Audit logs enabled
  â–¡ IP whitelisting configured

Compliance Certifications:
  âœ… SOC 2 Type II
  âœ… ISO 27001
  âœ… PCI-DSS (Service Provider Level 1)
  âœ… HIPAA
  âœ… GDPR compliant

SLO Recommendations:
  Core Banking: 99.95% availability
  Payment API: 99.99% availability
  Mobile API: 99.9% availability
  Internal Tools: 99.5% availability
```

---

## ğŸ“ TÃ³m Táº¯t

```
This is a REFERENCE, not a tutorial:
âœ“ Use for quick decisions
âœ“ Lookup common patterns
âœ“ Choose right tool for the job
âœ“ Troubleshoot issues

NOT for:
âœ— Step-by-step learning (see main docs)
âœ— Comprehensive understanding (see 01-19)
âœ— Copy-paste without understanding
```

---

**ğŸ¯ Knowledge Base - Always Here When You Need It**

Keep this reference handy for fast lookups and smart decisions!

---

**ğŸ“Œ Your Personal Quick Notes**
```
(Add your own shortcuts, decisions, patterns)








```
