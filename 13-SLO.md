# 13 - SERVICE LEVEL OBJECTIVES (SLO)

## üéØ M·ª•c Ti√™u
Hi·ªÉu s√¢u v·ªÅ SLO, SLA, SLI - b·∫£n ch·∫•t, khi n√†o d√πng, v√† c√°ch √°p d·ª•ng cho banking systems.

---

## üìö B·∫£n Ch·∫•t: SLI, SLO, SLA

### **Ba Concepts Li√™n Quan**

```
SLI (Service Level Indicator)
  ‚Üì ƒêo l∆∞·ªùng
SLO (Service Level Objective)  
  ‚Üì Cam k·∫øt
SLA (Service Level Agreement)
```

### **SLI - Service Level Indicator**

**ƒê·ªãnh nghƒ©a:**
> Metric ƒëo l∆∞·ªùng ch·∫•t l∆∞·ª£ng service t·ª´ g√≥c nh√¨n user

**V√≠ d·ª•:**
```
Request Success Rate:
  SLI = (Successful Requests / Total Requests) √ó 100

API Latency:
  SLI = p95 response time

Availability:
  SLI = (Uptime / Total Time) √ó 100
```

**Good SLI Characteristics:**
```
‚úì Measurable - C√≥ th·ªÉ ƒëo ƒë∆∞·ª£c ch√≠nh x√°c
‚úì Meaningful - Quan tr·ªçng v·ªõi users
‚úì Actionable - Team c√≥ th·ªÉ improve
‚úì Simple - D·ªÖ hi·ªÉu, d·ªÖ communicate

Example Banking SLIs:
- Transaction success rate
- ATM availability
- Mobile app login success rate
- Payment processing time
```

---

### **SLO - Service Level Objective**

**ƒê·ªãnh nghƒ©a:**
> Target value cho SLI - "Ch√∫ng ta mu·ªën ƒë·∫°t X%"

**Format:**
```
[SLI] should be [‚â•/‚â§] [Target] over [Time Window]

Examples:
- API success rate ‚â• 99.9% over 30 days
- p95 latency ‚â§ 200ms over 7 days
- Availability ‚â• 99.95% over 30 days
```

**T·∫°i Sao C·∫ßn SLO?**
```
Without SLO:
‚ùå "Service nhanh l√† ƒë∆∞·ª£c" - Vague
‚ùå Kh√¥ng c√≥ measurement of success
‚ùå Team kh√¥ng bi·∫øt prioritize g√¨

With SLO:
‚úÖ "p95 latency ‚â§ 200ms" - Concrete
‚úÖ Clear success criteria
‚úÖ Data-driven decisions
‚úÖ Balance reliability vs velocity
```

---

### **SLA - Service Level Agreement**

**ƒê·ªãnh nghƒ©a:**
> Contract v·ªõi customers - "N·∫øu kh√¥ng ƒë·∫°t, s·∫Ω c√≥ consequences"

**SLO vs SLA:**
```
SLO (Internal):
- Team's target
- Can be tighter than SLA
- Failure = Internal incident
- Example: 99.95% uptime

SLA (External):
- Customer commitment
- Usually looser than SLO
- Failure = Refund/penalty
- Example: 99.9% uptime

Why looser? Buffer zone (error budget)
```

**Banking Example:**
```
Service: Payment Processing API

SLI:
  - Success rate = successful payments / total attempts
  
SLO (Internal):
  - Success rate ‚â• 99.95% over 30 days
  - p95 latency ‚â§ 150ms
  
SLA (Customer-facing):
  - Success rate ‚â• 99.9% over 30 days
  - p95 latency ‚â§ 200ms
  - If violated: 10% monthly credit

Buffer: 0.05% for deployments, incidents
```

---

## üéØ Error Budget

### **Concept:**

**ƒê·ªãnh nghƒ©a:**
> L∆∞·ª£ng "failure ƒë∆∞·ª£c ph√©p" trong time window

**Calculation:**
```
Error Budget = 100% - SLO

Example:
SLO: 99.9% availability over 30 days
Error Budget: 0.1% = 43.2 minutes downtime allowed

30 days = 43,200 minutes
0.1% of 43,200 = 43.2 minutes
```

### **T·∫°i Sao Quan Tr·ªçng?**

**Balance Reliability vs Innovation:**
```
Scenario 1: Error Budget Remaining
  Status: SLO ƒëang ƒë·∫°t, c√≤n budget
  Decision: ‚úÖ Deploy new features
           ‚úÖ Experiment
           ‚úÖ Move fast
  
Scenario 2: Error Budget Exhausted
  Status: SLO b·ªã vi ph·∫°m, h·∫øt budget
  Decision: ‚õî Freeze deployments
           ‚õî Focus on reliability
           ‚õî Fix issues first
           
Result: Data-driven balance
```

**Banking Example:**
```
Core Banking API
SLO: 99.95% availability (21.6 min/month error budget)

Month Progress:
Week 1: 1 incident = 5 min downtime (23% budget used)
Week 2: Smooth (23% budget used)
Week 3: 2 incidents = 10 min (69% budget used)
Week 4 Decision:
  ‚úÖ Still have 31% budget ‚Üí Continue normal operations
  
If budget exhausted:
  ‚õî Freeze all changes
  ‚õî Only emergency fixes
  ‚õî Root cause analysis mandatory
```

---

## üè¶ Banking SLO Patterns

### **Availability SLOs**

**Transaction Processing:**
```
Target: 99.95% (21.6 min downtime/month)

Why 99.95% not 99.99%?
- Balance cost vs benefit
- 99.99% = 4x more expensive infrastructure
- 99.95% acceptable for non-critical hours
- Higher during business hours (time-based SLO)

Measurement:
SLI = (Successful requests / Total requests) √ó 100

Exclude from SLI:
- Scheduled maintenance (pre-announced)
- User errors (400 status codes)
- DDoS attacks (external factors)
```

**Time-Based SLO (Advanced):**
```
Business Hours (8 AM - 8 PM):
  SLO: 99.99% availability (52 seconds downtime/month)
  
Non-Business Hours (8 PM - 8 AM):
  SLO: 99.9% availability (5.2 min downtime/month)
  
Rationale: Higher impact during business hours
```

### **Latency SLOs**

**API Response Time:**
```
Why use percentiles, not average?

‚ùå Average: 100ms
   - Doesn't show user experience
   - 99% requests could be fast, 1% very slow
   
‚úÖ p95: 200ms
   - 95% of users experience ‚â§ 200ms
   - Captures long tail
   
‚úÖ p99: 500ms
   - 99% of users experience ‚â§ 500ms
   - Catches worst cases

Banking Standard:
- p95 ‚â§ 200ms (Good user experience)
- p99 ‚â§ 1000ms (Acceptable for complex transactions)
```

**Multi-Tier SLO:**
```
Payment API:

Tier 1 (Critical - Real-time payments):
  p95 ‚â§ 100ms
  p99 ‚â§ 200ms
  
Tier 2 (Standard - ACH transfers):
  p95 ‚â§ 500ms
  p99 ‚â§ 1000ms
  
Tier 3 (Batch - End-of-day):
  p95 ‚â§ 5000ms
  p99 ‚â§ 10000ms

Different SLOs for different use cases
```

### **Throughput SLOs**

**Transaction Capacity:**
```
SLO: Support 1000 TPS (Transactions Per Second) with p95 < 200ms

Why important for banking:
- Peak load planning (salary day, month-end)
- Capacity planning
- Scalability validation

Measurement:
- Synthetic tests: Can system handle 1000 TPS?
- Real load: Current TPS with latency tracking
```

### **Correctness SLOs**

**Data Accuracy:**
```
Banking Critical: 100% correctness required

SLO: 99.9999% transaction accuracy (Six Sigma)

What this means:
- 1 error per million transactions allowed
- For 100K transactions/day = 1 error per 10 days
- Any error triggers immediate investigation

Types of correctness:
- Amount accuracy (no rounding errors)
- Account balance consistency
- Transaction atomicity (no partial updates)
```

---

## üìä SLO Design Framework

### **Step 1: Choose SLIs**

**Questions:**
```
1. What matters to users?
   Banking: Speed, reliability, correctness
   
2. What can we measure?
   Metrics, logs, synthetic tests
   
3. What can we control?
   Our infrastructure, not third-party APIs (separate SLO)
```

**Banking SLI Selection:**
```
‚úÖ Good SLIs:
- Transaction success rate (we control)
- API response time (we control)
- ATM availability (we control)

‚ö†Ô∏è Questionable SLIs:
- Payment gateway success rate (third-party)
  ‚Üí Track separately, don't include in SLO
  
‚ùå Bad SLIs:
- User satisfaction score (subjective)
- Feature count (not user-facing)
```

### **Step 2: Set Targets**

**Approach:**
```
1. Historical Baseline
   - What are we achieving now?
   - Example: Currently 99.85% availability
   
2. User Expectations
   - What do users need?
   - Banking: Very high (99.9%+)
   
3. Cost Analysis
   - Cost of each nine
   - 99.9% vs 99.99% = 10x infrastructure cost
   
4. Competition Benchmark
   - What do others offer?
   - Industry standard for banking: 99.95%
   
5. Set Target
   - Slightly better than current
   - Meet user needs
   - Economically viable
   - Example: 99.9% ‚Üí reasonable target
```

**The Nines:**
```
Availability  | Downtime/Month | Downtime/Year | Cost Multiple
--------------|----------------|---------------|---------------
99%           | 7.2 hours      | 3.65 days     | 1x (baseline)
99.9%         | 43.2 minutes   | 8.76 hours    | 3-5x
99.95%        | 21.6 minutes   | 4.38 hours    | 5-10x
99.99%        | 4.3 minutes    | 52.6 minutes  | 10-100x
99.999%       | 26 seconds     | 5.26 minutes  | 100-1000x

Banking typical: 99.9% - 99.95%
Critical systems: 99.99% (payment switches)
```

### **Step 3: Define Time Window**

**Options:**
```
Rolling Window:
- Last 30 days
- Last 7 days
- Continuous evaluation
- Better for SRE teams

Calendar Window:
- This month
- This quarter
- Easier for business reporting
- Better for SLAs

Banking typically uses: 30-day rolling window
```

### **Step 4: Exclusions**

**What to Exclude:**
```
‚úÖ Exclude:
- Scheduled maintenance (announced 48h advance)
- User errors (400 status codes)
- External attacks (DDoS)
- Third-party service failures (clearly marked)

‚ùå Don't Exclude:
- Unscheduled downtime
- Our bugs
- Performance issues
- Capacity problems

Transparency principle: Only exclude what's truly outside control
```

---

## üö® SLO Monitoring & Alerting

### **Error Budget Alerts**

**Multi-Level Alerting:**
```
Level 1: Warning (50% budget consumed)
  Notify: Team Slack channel
  Action: Awareness, start reviewing
  
Level 2: Critical (80% budget consumed)
  Notify: Team + Manager
  Action: Freeze non-critical changes
  
Level 3: Emergency (100% budget consumed)
  Notify: All stakeholders + Exec
  Action: Full deployment freeze, war room
```

**Alert Example:**
```yaml
Alert: "Payment API Error Budget Critical"

Trigger: 
  Error budget consumed ‚â• 80% in current 30-day window
  
Message:
  "‚ö†Ô∏è Payment API Error Budget: 80% consumed
   
   Current SLO: 99.9% (43.2 min error budget)
   Used: 34.5 minutes
   Remaining: 8.7 minutes
   
   Actions:
   - Review recent incidents
   - Freeze non-critical deployments
   - Focus on reliability improvements
   
   Dashboard: [link]
   Postmortem: [link]"
```

### **Burn Rate Alerts**

**Concept:**
> How fast are we consuming error budget?

**Fast Burn (Critical):**
```
Consuming error budget at 10x rate
Example: Using 1 day budget in 2.4 hours

Alert: Immediate
Action: Stop everything, fix now
```

**Slow Burn (Warning):**
```
Consuming at 2x rate
Example: Using 1 day budget in 12 hours

Alert: Within 1 hour
Action: Investigate, plan fix
```

---

## üí° SLO Best Practices

### **1. Start Simple**

```
‚ùå Don't start with:
- 10 different SLIs
- Complex calculations
- Aggressive targets (99.99%)

‚úÖ Start with:
- 1-2 key SLIs (availability, latency)
- Simple measurements
- Achievable targets (current baseline + 10%)

Iterate and refine over time
```

### **2. User-Centric**

```
‚úÖ Good: "95% of login attempts succeed within 2 seconds"
   Why: User-facing, clear impact

‚ùå Bad: "Database CPU usage < 70%"
   Why: Internal metric, no direct user impact
   
Rule: If users don't notice, it's not an SLI
```

### **3. Realistic Targets**

```
Banking Trap: "We need 100% uptime!"

Reality:
- 100% is impossible
- Trying achieves 99.9% at 100x cost
- Better: 99.95% well-engineered system

Philosophy: "Reliability has diminishing returns"
```

### **4. Document Everything**

```
SLO Document Should Include:
- SLI definition and calculation
- Target and rationale
- Time window
- Exclusions policy
- Measurement method
- Owner team
- Review cadence (quarterly)
```

### **5. Regular Review**

```
Quarterly Review Questions:
- Is SLO still relevant?
- Too easy to achieve? (raise target)
- Always missing? (lower target or improve system)
- User expectations changed?
- Business requirements evolved?

SLOs should evolve with the service
```

---

## üè¶ Banking SLO Examples

### **Example 1: Core Banking System**

```yaml
Service: Account Management API
Owner: Core Banking Team

SLI 1: Availability
  Measurement: (Successful requests / Total requests) √ó 100
  Target: ‚â• 99.95%
  Window: 30-day rolling
  Exclusions: Scheduled maintenance, user errors (4xx)
  
SLI 2: Latency  
  Measurement: p95 response time
  Target: ‚â§ 200ms
  Window: 30-day rolling
  
Error Budget: 21.6 minutes/month
  
Alert Thresholds:
  - 50% budget consumed: Warning
  - 80% budget consumed: Critical
  - 100% budget consumed: Emergency freeze
```

### **Example 2: Payment Processing**

```yaml
Service: Payment Gateway
Owner: Payments Team

SLI 1: Transaction Success Rate
  Measurement: (Approved + Declined) / (Total - System Errors) √ó 100
  Target: ‚â• 99.99%
  Window: 7-day rolling (more stringent)
  Note: Declined by issuer = Success (correct behavior)
  
SLI 2: Processing Time
  Measurement: p99 end-to-end time
  Target: ‚â§ 3 seconds
  Window: 7-day rolling
  
SLI 3: Throughput
  Measurement: Sustained TPS
  Target: ‚â• 1000 TPS with p95 < 500ms
  Window: Peak hours (10 AM - 4 PM)
  
Error Budget: 1 minute/week
```

### **Example 3: Mobile Banking App**

```yaml
Service: Mobile API Backend
Owner: Digital Banking Team

SLI 1: Login Success Rate
  Measurement: (Successful logins / Attempted logins) √ó 100
  Target: ‚â• 99.9%
  Window: 30-day rolling
  Exclusions: Wrong password (user error)
  
SLI 2: API Availability
  Measurement: Synthetic test success rate
  Target: ‚â• 99.95%
  Window: 30-day rolling
  Tests: Every 1 minute from 5 global locations
  
SLI 3: Session Reliability
  Measurement: Sessions completed / Sessions started
  Target: ‚â• 99%
  Window: 7-day rolling
  
Time-Based SLO:
  Business hours (6 AM - 11 PM): All SLIs apply
  Night hours (11 PM - 6 AM): Relaxed to 99.5%
```

---

## üìä Implementing in Datadog

### **SLO Feature in Datadog**

**Create SLO:**
```
1. Navigate: Service Management ‚Üí SLO ‚Üí New SLO

2. Choose Type:
   - Metric-based (availability, latency)
   - Monitor-based (existing monitors)
   
3. Define SLI:
   Good events: success requests
   Total events: all requests
   
4. Set Target: 99.9% over 30 days

5. Alerts:
   - Warning: 50% error budget consumed
   - Critical: 80% consumed
```

**SLO Query Examples:**
```
Availability:
  SLI = sum:http.requests{status:2xx}.as_count() / 
        sum:http.requests{*}.as_count()
  Target: ‚â• 99.9%

Latency:
  SLI = p95:trace.http.request.duration{*}
  Target: ‚â§ 200ms
  
Success Rate:
  SLI = (sum:payments.success + sum:payments.declined) / 
        sum:payments.total
  Target: ‚â• 99.99%
```

### **SLO Dashboard**

**Recommended Widgets:**
```
1. SLO Status Overview
   - Current SLO % (Query Value)
   - Error budget remaining (Query Value)
   - Color: Green/Yellow/Red
   
2. Burn Rate Graph
   - How fast consuming budget (Timeseries)
   
3. Historical Performance
   - SLO over time (Timeseries)
   - Show target line
   
4. Error Budget Trend
   - Budget remaining over time
   - Project when will exhaust
   
5. Top Contributing Errors
   - What's causing SLO misses (Top List)
```

---

## ü§î Decision Framework

### **Khi N√†o Define SLO?**

```
‚úÖ Define SLO when:
- Service is user-facing
- Service is critical to business
- Need to balance reliability vs velocity
- Want data-driven decisions
- Multiple teams depend on service

‚ö†Ô∏è Maybe not needed:
- Internal tools with 5 users
- Services in alpha/beta
- Experimental features
```

### **How Many SLOs Per Service?**

```
‚úÖ Sweet spot: 2-3 SLOs per service

Example:
1. Availability (always)
2. Latency (for sync services)
3. Throughput or Correctness (as needed)

‚ùå Too many: Hard to manage, dilutes focus
‚úÖ Start with 1 (availability), add as mature
```

### **Internal vs External SLO?**

```
Pattern:
- Internal SLO: Tighter (99.95%)
- External SLA: Looser (99.9%)
- Buffer: 0.05% for safety margin

Rationale:
- Hit internal SLO = Customers happy
- Miss internal but hit SLA = Warning sign
- Miss SLA = Contract violation
```

---

## üìù T√≥m T·∫Øt

### **Key Takeaways**

```
1. SLI = Measurement
   - What to measure (success rate, latency)
   - User-centric, actionable
   
2. SLO = Target
   - What to achieve (99.9%, <200ms)
   - Internal goal
   
3. SLA = Contract
   - What to commit (99.5%)
   - External agreement
   
4. Error Budget = Innovation Fuel
   - Balance reliability vs velocity
   - Data-driven deployment decisions
   
5. Banking Standard
   - 99.9% - 99.95% typical
   - 99.99% for critical systems
   - Time-based SLOs for peak hours
```

### **Must Remember**

```
‚úì Perfect reliability is impossible
‚úì Higher nines = exponentially higher cost
‚úì SLOs should evolve with service
‚úì User-facing metrics only
‚úì Error budget enables innovation
‚úì Regular review and adjustment
```

---

## ‚û°Ô∏è B∆∞·ªõc Ti·∫øp Theo

**Related Topics:**
- [12 - Monitors & Alerts](12-MONITORS-ALERTS.md) - Implement SLO alerts
- [16 - Best Practices](16-BEST-PRACTICES.md) - SLO best practices
- [21 - Banking Implementation](21-BANKING-IMPLEMENTATION.md) - Banking SLO requirements

---

**üìå Ghi Ch√∫ C·ªßa B·∫°n**
```
(SLO definitions, targets, calculations cho projects c·ªßa b·∫°n)








```

