# 05 - DASHBOARD STRATEGY & DESIGN

## ğŸ¯ Má»¥c TiÃªu
Hiá»ƒu dashboard design principles, widget selection, query optimization vÃ  best practices Ä‘á»ƒ táº¡o dashboards hiá»‡u quáº£ cho stakeholders vÃ  teams.

---

## ğŸ“š Báº£n Cháº¥t: Dashboards LÃ  GÃ¬?

### **Purpose cá»§a Dashboards**

```
Dashboard â‰  Dump all metrics
Dashboard = Tell a story with data

3 loáº¡i stories:

1. Operational Story
   "Is the system healthy RIGHT NOW?"
   â†’ For: On-call engineers
   â†’ Real-time status, quick triage
   
2. Analytical Story
   "WHY did this happen?"
   â†’ For: Performance engineers
   â†’ Deep dive, correlation, trends
   
3. Business Story
   "What's the IMPACT?"
   â†’ For: Executives, product managers
   â†’ High-level KPIs, SLOs, business metrics
```

**Common Mistake:**
```
âŒ One dashboard for everyone
   â†’ Too complex for execs
   â†’ Not detailed enough for engineers
   
âœ… Purpose-built dashboards
   â†’ Executive dashboard (10 widgets, high-level)
   â†’ Engineering dashboard (50 widgets, detailed)
   â†’ On-call dashboard (20 widgets, actionable)
```

---

## ğŸ¨ Dashboard Types: When to Use What

### **Type 1: Timeboard**

**Definition:**
> Shared timeline across all widgets - zoom/pan affects everything

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [====== Shared Timeline: Last 4 Hours =====]â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CPU Usage          â”‚  Memory Usage        â”‚
â”‚  [graph synced]     â”‚  [graph synced]      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Request Rate       â”‚  Error Rate          â”‚
â”‚  [graph synced]     â”‚  [graph synced]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When to Use:**
```
âœ… Troubleshooting / Investigation
âœ… Correlation analysis
âœ… Incident response
âœ… Performance debugging

Example: "Response time spiked at 10 AM"
â†’ Timeboard shows CPU, memory, requests at 10 AM across all widgets
â†’ Easy to correlate
```

**Pros:**
- âœ… Zoom timeline â†’ all widgets adjust
- âœ… See correlations easily
- âœ… Great for "what happened when"

**Cons:**
- âš ï¸ All widgets must use same timeframe
- âš ï¸ Not flexible for mixed use cases

---

### **Type 2: Screenboard**

**Definition:**
> Independent timeline per widget - flexible layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Total Users (Last 24h)  â”‚  SLO (Last 30d) â”‚
â”‚  [independent time]      â”‚  [independent]  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Recent Deployments           â”‚  Text Note â”‚
â”‚  (Last 7 days)                â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When to Use:**
```
âœ… Executive dashboards
âœ… Status boards / NOC displays
âœ… Mixed timeframes needed
âœ… Public dashboards

Example: Executive Dashboard
â†’ SLO widget: Last 30 days
â†’ Revenue widget: Last 24 hours
â†’ Deploy widget: Last 7 days
â†’ Different timeframes make sense
```

**Pros:**
- âœ… Flexible layout (drag-and-drop)
- âœ… Each widget independent
- âœ… Can embed images, text, iframes

**Cons:**
- âš ï¸ No shared timeline (harder to correlate)

---

### **Decision Framework:**

```
Need to correlate events?
â”œâ”€ YES â†’ Timeboard
â”‚   Examples:
â”‚   - Troubleshooting dashboard
â”‚   - Performance analysis
â”‚   - Incident investigation
â”‚
â””â”€ NO â†’ Screenboard
    Examples:
    - Executive overview
    - Team status board
    - Mixed metrics (different timeframes)
```

---

## ğŸ§© Widget Selection Guide

### **Decision Tree: Which Widget?**

```
What are you showing?

â”œâ”€ Single number (current value)
â”‚  â””â”€ Use: Query Value
â”‚     Examples: Current CPU %, Active users, Error count
â”‚
â”œâ”€ Change over time
â”‚  â””â”€ Use: Timeseries
â”‚     Examples: Request rate, Response time trend, Memory usage
â”‚
â”œâ”€ Ranking (top/bottom N)
â”‚  â””â”€ Use: Top List
â”‚     Examples: Top hosts by CPU, Slowest endpoints, Most errors
â”‚
â”œâ”€ Distribution of values
â”‚  â”œâ”€ Histogram â†’ Use: Distribution
â”‚  â”‚  Examples: Latency percentiles (p50/p95/p99)
â”‚  â”‚
â”‚  â””â”€ Heatmap â†’ Use: Heatmap
â”‚     Examples: Request duration across services, Error patterns
â”‚
â”œâ”€ Real-time events
â”‚  â””â”€ Use: Log Stream
â”‚     Examples: Live errors, Deployment logs, Audit trail
â”‚
â”œâ”€ Infrastructure overview
â”‚  â””â”€ Use: Host Map
â”‚     Examples: Fleet health, Resource utilization across hosts
â”‚
â”œâ”€ Service relationships
â”‚  â””â”€ Use: Service Map
â”‚     Examples: Microservice dependencies, Data flow
â”‚
â””â”€ Multiple metrics comparison
   â””â”€ Use: Table
      Examples: Host comparison (CPU, RAM, Disk), Service comparison
```

---

### **Widget Deep Dive**

#### **Query Value - Single Number**

**Purpose:** Show current state or aggregate value

**Use Cases:**
```
âœ… Current CPU usage: 67.5%
âœ… Total requests today: 1.2M
âœ… Active users: 850
âœ… Error rate: 0.3%
```

**Design Considerations:**
```yaml
Conditional Formatting (Critical):
  < 50%: Green (good)
  50-80%: Yellow (warning)
  > 80%: Red (critical)
  
Unit Selection:
  - Percentage: %
  - Duration: ms, s
  - Throughput: /s, /min
  - Count: format with commas (1,234,567)

Font Size:
  - Large for dashboards on TVs
  - Smaller for dense dashboards
```

**Common Mistake:**
```
âŒ Query Value for trend data
   Example: response_time query value = 250ms
   â†’ Missing: Is it getting better or worse?
   
âœ… Use Query Value + Timeseries
   â†’ Value shows current: 250ms
   â†’ Timeseries shows trend: increasing!
```

---

#### **Timeseries - Line/Area/Bar Chart**

**Purpose:** Show changes over time

**Visualization Options:**
```
Lines:
  âœ… Multiple series comparison
  âœ… Sparse data
  Example: Compare CPU across 5 hosts

Area:
  âœ… Show cumulative effect
  âœ… Emphasize magnitude
  Example: Total requests (stacked by service)

Bars:
  âœ… Discrete events
  âœ… Clear separation
  Example: Deployments per day, Errors per hour
```

**Display Modes:**
```yaml
Lines:
  - Interpolation: linear, step, cardinal
  - Line width: thin (multiple series), thick (single series)
  
Bands (with lines):
  - Show bounds/uncertainty
  - Example: Anomaly detection bounds
  
Both:
  - Combination for clarity
```

**Y-Axis Decisions:**
```
Auto-scale (default):
  âœ… Good for exploration
  âš ï¸ Can be misleading (exaggerates changes)
  
Fixed scale:
  âœ… Consistent interpretation
  âœ… Compare across dashboards
  Example: CPU always 0-100%, not auto-scale 60-80%
  
Log scale:
  âœ… When range varies greatly
  Example: Error counts (1 to 10,000)
```

---

#### **Top List - Rankings**

**Purpose:** Show top/bottom N items

**Use Cases:**
```
âœ… Top 10 hosts by CPU
âœ… Slowest 5 endpoints
âœ… Most active users (careful: cardinality!)
âœ… Largest database tables
```

**Configuration:**
```yaml
Order:
  Descending: Top N highest
  Ascending: Bottom N (lowest)
  
Limit:
  Default: 10
  Consideration: More = harder to read
  
Conditional Format:
  Highlight top 1 in red (if bad metric)
  Highlight top 1 in green (if good metric)
```

**Gotcha:**
```
âŒ Top list by high-cardinality tag
   Example: Top 100 users by requests
   â†’ Creates 100 custom metrics
   â†’ Expensive!
   
âœ… Top list by low-cardinality tag
   Example: Top 10 services by requests
   â†’ Max 10 metrics
```

---

#### **Heatmap - Pattern Visualization**

**Purpose:** Show distribution across dimensions

**Use Cases:**
```
âœ… Response time distribution across services
âœ… Error patterns by hour of day
âœ… Request volume by region and time
```

**When to Use:**
```
âœ… When you have 2 dimensions
   Example: Service (X) Ã— Time (Y)
   
âœ… When you want to see patterns
   Example: "Service A always slow at midnight"
   
âœ… When exact numbers less important than trends
```

**Color Schemes:**
```
Sequential (default):
  - Light â†’ Dark (low â†’ high)
  - Good for most use cases
  
Diverging:
  - Blue â† Neutral â†’ Red
  - Good for deltas (negative/positive)
```

---

## ğŸ” Query Language Mastery

### **Query Anatomy**

```
<aggregation>:<metric>{<filters>} [by {<tags>}] [functions]
     â–²           â–²         â–²            â–²           â–²
     â”‚           â”‚         â”‚            â”‚           â”‚
   What to   Which    Which       Group by     Transform
     do      metric   subset       what
```

### **Aggregations: What To Do**

```
avg:   Average (most common)
sum:   Total (for counts, requests)
min:   Minimum value
max:   Maximum value (peak)
count: Number of data points
```

**When to use:**
```yaml
avg:
  - CPU usage (average across time)
  - Response time (mean)
  - Memory usage
  
sum:
  - Request counts (total requests)
  - Error counts (total errors)
  - Bytes transferred (total bandwidth)
  
max:
  - Peak CPU (worst case)
  - Max response time (slowest request)
  - Highest queue depth
  
min:
  - Available disk space (lowest point)
  - Minimum connections (baseline)
```

---

### **Filters: Which Subset**

```bash
# Single tag
{env:production}

# Multiple tags (AND)
{env:production,service:api}

# OR logic
{env:production OR env:staging}
{service:api OR service:worker}

# Wildcards
{service:web-*}
{host:prod-db-*}

# NOT
{env:production,-service:test}
```

**Best Practice:**
```yaml
Specific is better:
âŒ {*}  # All data â†’ expensive, slow
âœ… {env:production,service:payment-api}  # Precise

Use template variables:
âŒ Hardcoded: {env:production}
âœ… Dynamic: {env:$env}
   â†’ Users can switch env via dropdown
```

---

### **Group By: Split Data**

```bash
# Single dimension
avg:system.cpu.user{*} by {host}
â†’ One line per host

# Multiple dimensions
sum:http.requests{*} by {service,status_code}
â†’ One line per service+status combo
```

**Cardinality Warning:**
```
Group by creates: N Ã— M series

Example:
- 10 services
- 5 status codes (200, 400, 404, 500, 503)
= 50 series in one graph

Too many series â†’ unreadable graph
Limit: Keep under 20-30 series per graph
```

---

### **Functions: Transform Data**

#### **as_rate() / as_count()**

```bash
# Convert count to rate (per second)
sum:http.requests{*}.as_rate()
â†’ Requests per second

# Convert rate to count
sum:http.requests{*}.as_count()
â†’ Total requests in timeframe
```

**When to use:**
```
as_rate():
  âœ… Counts that accumulate (requests, errors)
  âœ… Want per-second rate for comparison
  
as_count():
  âœ… See total over period
  Example: Total errors in last hour
```

---

#### **rollup() - Time Aggregation**

```bash
# Average over custom window
avg:system.cpu.user{*}.rollup(avg, 60)
â†’ Average CPU in 60-second windows

# Sum over window
sum:http.requests{*}.rollup(sum, 300)
â†’ Total requests per 5 minutes
```

**Use Case:**
```
Long timeframes (days/weeks):
â†’ Default rollup = 1 point per pixel
â†’ Can miss spikes

Custom rollup:
â†’ rollup(max, 60)
â†’ Shows peak value in each window
â†’ Won't miss spikes
```

---

#### **timeshift() - Compare to Past**

```bash
# Compare to yesterday
avg:system.cpu.user{*}
timeshift(avg:system.cpu.user{*}, 86400)

# Compare to last week
timeshift(avg:system.cpu.user{*}, 604800)
```

**Seconds conversion:**
```
1 hour   = 3600
1 day    = 86400
1 week   = 604800
1 month  = 2592000 (30 days)
```

**Use Case:**
```
Banking example:
- Compare transaction volume today vs last Monday
- Identify weekly patterns
- Validate capacity planning
```

---

#### **anomalies() - ML Detection**

```bash
anomalies(avg:system.cpu.user{*}, 'basic', 2)
           â”‚                        â”‚       â”‚
           metric                algorithm bounds
```

**Algorithms:**
```
basic:
  - Fast, simple
  - Good for stable metrics
  
agile:
  - Adapts quickly to changes
  - Good for dynamic metrics
  
robust:
  - Ignores outliers
  - Good for noisy data
```

**Bounds:**
```
2 = 2 standard deviations
  â†’ Moderate sensitivity
  
3 = 3 standard deviations
  â†’ Lower sensitivity (fewer false positives)
  
1 = 1 standard deviation
  â†’ High sensitivity (more alerts)
```

---

#### **forecast() - Predict Future**

```bash
forecast(avg:system.disk.used{*}, 'linear', 1w)
         â”‚                         â”‚         â”‚
         metric                  algorithm  horizon
```

**Use Case:**
```
Capacity planning:
- When will disk be full?
- When to add more servers?
- Budget planning
```

**Banking Example:**
```
Transaction growth:
forecast(sum:transactions{*}.as_rate(), 'linear', 1mo)

Shows: "At current rate, will hit 10K TPS in 3 weeks"
Action: Plan infrastructure upgrade
```

---

## ğŸ¯ Dashboard Design Patterns

### **Pattern 1: RED Method Dashboard**

**For: Service/API monitoring**

```
RED = Rate, Errors, Duration

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Service: Payment API                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Rate (Traffic)                              â”‚
â”‚  - Requests per second                       â”‚
â”‚  - Timeseries: sum:requests{*}.as_rate()    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Errors (Error Rate)                         â”‚
â”‚  - Error percentage                          â”‚
â”‚  - Query: (errors / total) * 100            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Duration (Latency)                          â”‚
â”‚  - p50, p95, p99 response time              â”‚
â”‚  - Distribution widget                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why RED:**
- âœ… Complete service health picture
- âœ… Industry standard
- âœ… Easy to understand

---

### **Pattern 2: USE Method Dashboard**

**For: Infrastructure monitoring**

```
USE = Utilization, Saturation, Errors

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Resource: CPU                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Utilization                                 â”‚
â”‚  - % of CPU used                             â”‚
â”‚  - avg:system.cpu.user{*}                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Saturation                                  â”‚
â”‚  - Load average, queue depth                 â”‚
â”‚  - avg:system.load.1{*}                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Errors                                      â”‚
â”‚  - CPU throttling, context switches          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Apply to:**
- CPU, Memory, Disk, Network
- Database connections
- Queue systems

---

### **Pattern 3: Golden Signals (Google SRE)**

```
4 Golden Signals: Latency, Traffic, Errors, Saturation

Similar to RED + USE, but adds:
- Saturation (resource fullness)

Banking implementation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Latency: p95 transaction time < 200ms      â”‚
â”‚  Traffic: 1000 TPS current                  â”‚
â”‚  Errors: 0.1% error rate                    â”‚
â”‚  Saturation: Database connections 70% used  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Pattern 4: Hierarchical (Top-Down)**

```
Executive View â†’ Team View â†’ Service View â†’ Resource View

Example: Banking Dashboard Hierarchy

Level 1 (Executive):
â”œâ”€ Overall SLO: 99.95%
â”œâ”€ Total Transactions: 1.2M today
â”œâ”€ Revenue: $500K today
â””â”€ Critical Incidents: 0

Level 2 (Team Lead):
â”œâ”€ Service Status (all services)
â”œâ”€ Team-specific SLOs
â”œâ”€ Recent Deployments
â””â”€ Top Issues

Level 3 (Engineer):
â”œâ”€ Detailed service metrics
â”œâ”€ Error breakdown
â”œâ”€ Performance deep dive
â””â”€ Resource utilization

Level 4 (Deep Dive):
â”œâ”€ Individual host metrics
â”œâ”€ Database queries
â”œâ”€ Log streams
â””â”€ Trace analysis
```

---

## ğŸ¨ Design Best Practices

### **1. Layout & Organization**

**Grid System:**
```
âœ… Use consistent widget sizes
   - Full width for titles
   - 50% width for paired metrics
   - 33% width for trios
   
âŒ Random sizes and placement
   - Hard to scan
   - Looks unprofessional
```

**Logical Flow:**
```
Top to Bottom priority:
1. Most important metrics (top)
2. Supporting context (middle)
3. Deep dive / details (bottom)

Left to Right:
1. Input metrics (left)
2. Processing metrics (middle)
3. Output metrics (right)

Example: API Dashboard
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request Rate â”‚ Process Time â”‚ Response Rateâ”‚
â”‚ (Input)      â”‚ (Middle)     â”‚ (Output)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **2. Color Usage**

**Standard Colors:**
```
Green: Good / Normal / Success
Yellow: Warning / Degraded
Red: Critical / Error / Alert
Blue: Info / Neutral
Purple: Custom / Secondary metric
```

**Consistency:**
```
âœ… Same colors for same concepts across all dashboards
   - Errors always red
   - Success always green
   
âŒ Random colors
   - Errors sometimes red, sometimes orange
   - Confusing for teams
```

**Color Blindness:**
```
âš ï¸ 8% of men are color blind (red-green most common)

Accessible approach:
âœ… Use shapes + colors
âœ… Use text labels
âœ… Use patterns (solid, dashed, dotted)
```

---

### **3. Naming Conventions**

**Dashboard Names:**
```
Pattern: [Team/System] - [Purpose]

Examples:
âœ… "Payment API - Production Health"
âœ… "Core Banking - Executive Overview"
âœ… "Infrastructure - On-Call Dashboard"

âŒ "Dashboard 1"
âŒ "John's Dashboard"
âŒ "Temp Dashboard"
```

**Widget Titles:**
```
Pattern: [Metric Name] - [Context]

Examples:
âœ… "Response Time - p95 (Last 1 hour)"
âœ… "Error Rate % - By Service"
âœ… "Database Connections - PostgreSQL Primary"

âŒ "Graph 1"
âŒ "Metric"
âŒ "system.cpu.user"
```

---

### **4. Context & Documentation**

**Add Notes Widgets:**
```
Purpose:
- Explain what metrics mean
- Document thresholds
- Link to runbooks
- Provide business context

Example:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“ Response Time Targets                   â”‚
â”‚                                             â”‚
â”‚ Target: p95 < 200ms                        â”‚
â”‚ Warning: p95 > 200ms                       â”‚
â”‚ Critical: p95 > 500ms                      â”‚
â”‚                                             â”‚
â”‚ Runbook: wiki.company.com/api-slow         â”‚
â”‚ On-call: Slack #team-backend              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Use Markdown:**
```markdown
## Response Time SLO

**Target**: p95 < 200ms  
**Current**: 185ms âœ…

### Actions if breached:
1. Check database slow query log
2. Review recent deployments
3. Escalate to @backend-lead

[Runbook](https://wiki.company.com/slo-breach)
```

---

### **5. Template Variables**

**Purpose:**
```
Make dashboards reusable across:
- Environments (prod, staging, dev)
- Services (api, web, worker)
- Teams (backend, frontend, data)
```

**Implementation:**
```yaml
Variable 1: $env
  Tag: env
  Default: production
  Values: production, staging, development

Variable 2: $service
  Tag: service
  Default: *
  Values: (auto from tag)

Variable 3: $region
  Tag: region
  Default: *
  Values: us-east-1, us-west-2, eu-west-1

Usage in queries:
avg:system.cpu.user{env:$env,service:$service,region:$region}
```

**User Experience:**
```
Before variables:
- Need separate dashboard per env = 3 dashboards
- Change service = edit every query

With variables:
- 1 dashboard for all envs
- Dropdown to switch env/service
- No editing needed
```

---

## âš ï¸ Common Mistakes

### **Mistake 1: Too Many Widgets**

```
âŒ Problem: 100 widgets on one dashboard
   â†’ Information overload
   â†’ Slow load time
   â†’ Nothing stands out
   
âœ… Solution: Multiple focused dashboards
   Dashboard 1: Overview (10 widgets)
   Dashboard 2: Deep Dive CPU (20 widgets)
   Dashboard 3: Deep Dive Network (20 widgets)
```

---

### **Mistake 2: No Aggregation**

```
âŒ Problem: 
   Query: avg:system.cpu.user{*} by {host}
   Result: 1000 lines (one per host)
   â†’ Unreadable

âœ… Solution: Aggregate appropriately
   Option 1: avg across all hosts
     avg:system.cpu.user{*}
   
   Option 2: Top 10 only
     Top List widget
     
   Option 3: Percentiles
     p95:system.cpu.user{*}
```

---

### **Mistake 3: Wrong Timeframe**

```
âŒ CPU dashboard showing last 5 years
   â†’ Can't see recent spikes
   
âœ… Match timeframe to use case:
   Real-time monitoring: Last 1 hour
   Daily operations: Last 4 hours
   Trends: Last 7 days
   Capacity planning: Last 30 days
```

---

### **Mistake 4: No Thresholds**

```
âŒ Just show metrics
   â†’ Users don't know if good or bad
   
âœ… Add context:
   - Horizontal line at threshold
   - Conditional formatting
   - Notes with targets
   
Example: CPU graph
- Add red line at 80% (critical)
- Add yellow line at 60% (warning)
- Note: "Target: < 60% average"
```

---

## ğŸ“Š Banking Dashboard Examples

### **Executive Dashboard**

```yaml
Purpose: High-level business + tech metrics
Audience: CTO, VP Engineering, Business Leaders
Refresh: 5 minutes
Type: Screenboard

Widgets (12 total):

Row 1 (KPIs):
  - Total Transactions Today (Query Value)
  - Transaction Success Rate % (Query Value, conditional format)
  - Average Response Time (Query Value)
  - SLO Status - 99.9% (SLO Widget)

Row 2 (Trends):
  - Transactions Per Hour (Timeseries)
  - Revenue Per Hour (Timeseries, business metric)
  
Row 3 (Ops):
  - Active Incidents (Query Value)
  - Recent Deployments (Event Stream)
  - System Health (Host Map)
  
Row 4 (Notes):
  - Status Message (Text: "All systems operational âœ…")
  - On-Call Contact (Text: current on-call engineer)
```

---

### **On-Call Dashboard**

```yaml
Purpose: Quick triage and incident response
Audience: On-call engineer
Refresh: 1 minute (auto)
Type: Timeboard (shared timeline for correlation)

Widgets (20 total):

Critical Alerts (Top):
  - Active Alerts (Monitor Status Widget)
  - Error Rate % (Query Value, big, red if > 1%)
  
Service Health:
  - Request Rate (Timeseries)
  - Response Time p95/p99 (Timeseries)
  - Error Count (Timeseries)
  - Top Errors by Endpoint (Top List)
  
Infrastructure:
  - CPU Usage by Host (Timeseries)
  - Memory Usage by Host (Timeseries)
  - Disk Usage (Host Map)
  
Logs:
  - Error Logs (Log Stream, live)
  - Recent Deployments (Event Timeline)
  
Context:
  - Runbook Links (Text widget)
  - Escalation Contacts (Text widget)
```

---

### **Service Deep Dive**

```yaml
Service: Payment Processing API
Purpose: Detailed performance analysis
Audience: Payment team engineers
Type: Timeboard

Widgets:

RED Metrics:
  - Request Rate (total, by endpoint)
  - Error Rate (total, by error type)
  - Latency Distribution (p50/p75/p95/p99)
  
Dependencies:
  - Database Query Time
  - External API Calls (payment gateway)
  - Cache Hit Rate
  
Resources:
  - CPU per Container
  - Memory per Container
  - Network I/O
  
Business Metrics:
  - Transaction Amount (sum)
  - Average Transaction Size
  - Payment Methods Distribution
  
Traces & Logs:
  - Slow Traces (> 1s)
  - Error Logs Stream
  - Deployment Events
```

---

## ğŸ“ TÃ³m Táº¯t

### **Key Decisions:**

```
1. Dashboard Type
   Timeboard: Troubleshooting, correlation
   Screenboard: Overview, mixed timeframes
   
2. Widget Selection
   Match widget to data type
   Don't force wrong widget
   
3. Query Design
   Aggregate appropriately
   Use template variables
   Add context (thresholds, notes)
   
4. Layout
   Prioritize: Top = most important
   Group related metrics
   Consistent sizing
   
5. Audience
   Different dashboards for different audiences
   Executive â‰  Engineer dashboards
```

### **Best Practices:**

```
âœ… Purpose-built dashboards
âœ… Clear naming conventions
âœ… Add context (notes, thresholds)
âœ… Use template variables
âœ… Conditional formatting
âœ… Limit widgets (10-30 per dashboard)
âœ… Document in notes widgets
```

### **Banking Specifics:**

```
âœ… SLO widgets prominent
âœ… Business metrics included
âœ… Compliance/audit trail
âœ… Multi-region views
âœ… Separate dashboards for different stakeholders
```

---

## â¡ï¸ BÆ°á»›c Tiáº¿p Theo

**Related Topics:**
- [06 - Metrics Design](06-METRICS.md) - Metric types, naming
- [12 - Monitors & Alerts](12-MONITORS-ALERTS.md) - Alert from dashboard metrics
- [13 - SLO](13-SLO.md) - SLO widgets and tracking

---

**ğŸ“Œ Ghi ChÃº Cá»§a Báº¡n**
```
(Dashboard designs, widget decisions, lessons learned)








```
